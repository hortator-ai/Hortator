# ============================================================================
# Hortator Helm Values — Sane Defaults
# ============================================================================
# Override via: helm install --values my-overrides.yaml
#               helm install --set key=value
#
# Three-tier override: Helm defaults → AgentRole CRD → AgentTask CRD
# ============================================================================

# ── Operator ────────────────────────────────────────────────────────────────

operator:
  image: ghcr.io/hortator/operator:latest
  replicas: 1
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 256Mi
  # Reconciliation interval for cleanup, health checks, budget enforcement
  reconcileIntervalSeconds: 30

# ── Default Runtime ─────────────────────────────────────────────────────────

runtime:
  # Default container image for agent Pods (when AgentTask.spec.image is omitted)
  image: ghcr.io/hortator/runtime:latest

  # Default resource limits for agent Pods (AgentTask.spec.resources overrides)
  resources:
    requests:
      cpu: 250m
      memory: 256Mi
    limits:
      cpu: "1"
      memory: 1Gi

  # Default task timeout in seconds (AgentTask.spec.timeout overrides)
  defaultTimeoutSeconds: 600

  # Filesystem conventions
  filesystem:
    enforceRequired: true         # Fail task if /outbox/result.json missing at completion
    conventions:
      decisionsLog: true          # Runtime auto-maintains /memory/decisions.log
      errorsLog: true             # Runtime auto-maintains /memory/errors.log

  # Context management
  contextManagement:
    strategy: structured          # structured | summarize | hybrid
    structured:
      stateFile: /memory/state.json
      autoExtract: true
    summarization:
      enabled: true               # Fallback when context exceeds threshold
      triggerPercent: 75           # Summarize when 75% of context window used
      keepRecentTurns: 10         # Always keep last N turns verbatim
    vectorRetrieval:
      enabled: false              # Opt-in, requires vector DB
      # backend: pgvector | milvus | qdrant | chroma
      # endpoint: ""

# ── Models / LLM Routing ───────────────────────────────────────────────────

models:
  # Default LLM endpoint (used when AgentTask/AgentRole don't specify)
  default:
    endpoint: ""                  # Required: user must configure
    name: ""
    apiKeyRef: {}                 # secretName + key

  # Convenience presets for common in-cluster LLM backends
  presets:
    ollama:
      enabled: false
      endpoint: http://ollama.default.svc:11434/v1
    vllm:
      enabled: false
      endpoint: http://vllm.default.svc:8000/v1
    litellm:
      enabled: false              # Ties into budget.litellmProxy
      endpoint: http://litellm.default.svc:4000/v1

# ── Budget / Cost Tracking ──────────────────────────────────────────────────

budget:
  enabled: true

  # Price source for token → cost calculation
  priceSource: litellm            # litellm | custom
  refreshIntervalHours: 24        # How often to fetch LiteLLM price map from GitHub
  fallbackBehavior: track-tokens  # track-tokens | block | warn
                                  # track-tokens: log usage even if model not in price map
                                  # block: refuse LLM calls for unknown models
                                  # warn: log warning + track tokens

  # Custom/override prices (merged on top of LiteLLM price map)
  # Per million tokens: [input, output]
  customPrices: {}
    # my-custom-model: { inputPerMillion: 1.0, outputPerMillion: 3.0 }
    # local/ollama: { inputPerMillion: 0, outputPerMillion: 0 }

  # Default per-task budget (AgentTask.spec.budget overrides)
  defaultLimit:
    maxCostUsd: "1.00"

  # Optional: LiteLLM proxy for authoritative cost tracking (enterprise)
  litellmProxy:
    enabled: false
    # Deploy as sub-chart, or point to existing instance
    # chart: litellm/litellm-proxy
    # endpoint: http://litellm.default.svc:4000

# ── Presidio / PII Detection ───────────────────────────────────────────────

presidio:
  enabled: true
  image: mcr.microsoft.com/presidio-analyzer:latest
  model: en_core_web_sm
  scoreThreshold: 0.5
  action: redact                  # redact | detect | hash | mask

  recognizers:
    disabled:
      # PhoneRecognizer iterates patterns for every country code on every
      # request, making it by far the slowest built-in recognizer (~10x others).
      # If you need phone detection, enable it and restrict to specific
      # countries via custom config to limit the performance impact.
      - PhoneRecognizer
    custom:
      - name: AWSKeyRecognizer
        entity: AWS_ACCESS_KEY
        patterns:
          - regex: "AKIA[0-9A-Z]{16}"
            score: 0.95
      - name: BearerTokenRecognizer
        entity: BEARER_TOKEN
        patterns:
          - regex: "Bearer [A-Za-z0-9\\-._~+/]+=*"
            score: 0.9
      - name: PrivateKeyRecognizer
        entity: PRIVATE_KEY
        patterns:
          - regex: "-----BEGIN (RSA |EC |DSA )?PRIVATE KEY-----"
            score: 0.99

  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi

# ── Telemetry / Audit Events ───────────────────────────────────────────────

telemetry:
  enabled: true

  # OTel Collector — deploy as sub-chart or use existing
  collector:
    deploy: true                  # true = deploy OTel Collector via sub-chart
    # If false, operator emits OTLP to this endpoint:
    # endpoint: http://otel-collector.monitoring:4317

  # Exporters (configure based on your observability stack)
  exporters:
    otlp:
      endpoint: ""                # User fills in (e.g. Datadog, Grafana Cloud)
    # prometheus:
    #   enabled: true             # Expose /metrics on operator
    # logging:
    #   enabled: false            # Stdout fallback for debugging

# ── Health / Stuck Detection ────────────────────────────────────────────────

health:
  enabled: true
  checkIntervalSeconds: 30

  stuckDetection:
    enabled: true
    defaults:
      toolDiversityMin: 0.3       # < 0.3 = repetitive actions (likely stuck)
      maxRepeatedPrompts: 3       # > 3 similar prompts in window = looping
      statusStaleMinutes: 5       # No self-reported progress update
      checkWindowMinutes: 5       # Sliding window for behavioral analysis
      action: warn                # warn | kill | escalate

    # Per-role overrides — different roles have different "normal" behavior
    roleOverrides: {}
      # qa-engineer:
      #   toolDiversityMin: 0.15    # Test loops are expected behavior
      #   maxRepeatedPrompts: 6     # Retrying tests is normal
      # researcher:
      #   statusStaleMinutes: 10    # Reading takes longer, fewer tool calls

    # Per-tier defaults (tribune/centurion/legionary)
    tierOverrides: {}
      # legionary:
      #   action: kill              # Kill stuck legionaries, don't waste tokens
      # centurion:
      #   action: escalate          # Escalate stuck centurions to tribune
      #   statusStaleMinutes: 10    # Coordination takes longer

  metrics:
    expose: true                  # Prometheus endpoint on operator
    prefix: hortator              # Metric name prefix

# ── Storage ─────────────────────────────────────────────────────────────────

storage:
  # Default storage class for PVCs (empty = cluster default)
  storageClass: ""
  defaultSize: "1Gi"

  # TTL-based cleanup
  cleanup:
    ttl:
      completed: 7d              # PVCs from completed tasks
      failed: 2d                 # PVCs from failed tasks
      cancelled: 1d              # PVCs from cancelled tasks

  # Namespace quota guardrail
  quota:
    enabled: true
    maxPerNamespace: 50Gi
    warningPercent: 80
    evictionPolicy: oldest-completed

  # Retained PVC knowledge discovery
  retained:
    discovery: tags               # none | tags | semantic (post-MVP)
    autoMount: true               # Mount matching retained PVCs into new tasks
    mountMode: readOnly
    staleAfterDays: 90
    maxRetainedPerNamespace: 20
    graduation:
      vectorStore:
        enabled: false            # Opt-in: index stale PVCs into vector DB before deletion
        # backend: pgvector | milvus | qdrant
        # endpoint: ""
      objectStore:
        enabled: false            # Opt-in: archive to S3/MinIO before deletion
        # bucket: hortator-archives
        # prefix: "{{ .Namespace }}/{{ .TaskId }}/"

# ── Security ────────────────────────────────────────────────────────────────

security:
  # Default capabilities for all tasks (AgentTask.spec.capabilities overrides)
  defaultCapabilities:
    - shell

  # Capability → NetworkPolicy mapping
  networkPolicies:
    enabled: true
    # web-fetch: allow egress to internet
    # shell: allow egress to cluster DNS only
    # spawn: allow egress to K8s API server

  # RBAC: legionaries get minimal permissions (create AgentTasks in own namespace only)
  rbac:
    enabled: true

# ── Enterprise ──────────────────────────────────────────────────────────────

enterprise:
  enabled: false                  # Switches to enterprise image with additional features
  image: ghcr.io/hortator/operator:enterprise
  # license:
  #   secretRef:
  #     name: hortator-license
  #     key: license-key

# ── Examples / Quickstart ───────────────────────────────────────────────────

examples:
  enabled: false                  # Don't install by default — keep production clean
  namespace: hortator-demo        # Separate namespace for easy cleanup
  # When enabled, installs:
  # - Sample ClusterAgentRoles (backend-dev, qa-engineer)
  # - "Hello world" AgentTask
  # Same manifests available standalone: kubectl apply -f examples/quickstart/
